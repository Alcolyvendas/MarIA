{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "92b16ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: [Errno 2] No such file or directory: './dataEncoderDecoder_item_id.pkl'\n",
      "       item_id                                     item_descrip  \\\n",
      "0            2       Camiseta Algodón Manga larga Primavera (2)   \n",
      "1            3         Blusa Seda Manga corta Casual Verano (3)   \n",
      "2            4        Sudadera Denim Largo Casual Primavera (4)   \n",
      "3            7                   Vestido Cuero Corto Verano (7)   \n",
      "4            8      Camiseta Seda Completo Casual Primavera (8)   \n",
      "...        ...                                              ...   \n",
      "23686    28139   Sandalias Cuero Completo Casual Verano (28139)   \n",
      "23687    28140           Camiseta Piel sintética Verano (28140)   \n",
      "23688    28141  Leggings Lino Amortiguada Cóctel Verano (28141)   \n",
      "23689    28142           Botas Plástico Ajustable Otoño (28142)   \n",
      "23690    28143            Camiseta Seda Completo Verano (28143)   \n",
      "\n",
      "                                           item_keywords  \n",
      "0      12-14 camiseta liso m primavera manga larga co...  \n",
      "1      12-14 transpirable blusa mujer manga corta chi...  \n",
      "2      mango con capucha sudadera xs largo denim 12-1...  \n",
      "3      22-24 l mujer verano vestido transpirable cort...  \n",
      "4      12-14 mango completo transpirable liso primave...  \n",
      "...                                                  ...  \n",
      "23686  uni sandalias completo verano cuero 12-14 homb...  \n",
      "23687       piel sintética 12-14 verano camiseta mango s  \n",
      "23688  12-14 liso cóctel leggings lino mango amortigu...  \n",
      "23689  22-24 ajustable bebe con cinturón botas poloni...  \n",
      "23690  12-14 turquía hombre con protección uv complet...  \n",
      "\n",
      "[23691 rows x 3 columns]\n",
      "       item_id                                     item_descrip  \\\n",
      "0            2       Camiseta Algodón Manga larga Primavera (2)   \n",
      "1            3         Blusa Seda Manga corta Casual Verano (3)   \n",
      "2            4        Sudadera Denim Largo Casual Primavera (4)   \n",
      "3            7                   Vestido Cuero Corto Verano (7)   \n",
      "4            8      Camiseta Seda Completo Casual Primavera (8)   \n",
      "...        ...                                              ...   \n",
      "23686    28139   Sandalias Cuero Completo Casual Verano (28139)   \n",
      "23687    28140           Camiseta Piel sintética Verano (28140)   \n",
      "23688    28141  Leggings Lino Amortiguada Cóctel Verano (28141)   \n",
      "23689    28142           Botas Plástico Ajustable Otoño (28142)   \n",
      "23690    28143            Camiseta Seda Completo Verano (28143)   \n",
      "\n",
      "                                           item_keywords  \n",
      "0      12-14 camiseta liso m primavera manga larga co...  \n",
      "1      12-14 transpirable blusa mujer manga corta chi...  \n",
      "2      mango con capucha sudadera xs largo denim 12-1...  \n",
      "3      22-24 l mujer verano vestido transpirable cort...  \n",
      "4      12-14 mango completo transpirable liso primave...  \n",
      "...                                                  ...  \n",
      "23686  uni sandalias completo verano cuero 12-14 homb...  \n",
      "23687       piel sintética 12-14 verano camiseta mango s  \n",
      "23688  12-14 liso cóctel leggings lino mango amortigu...  \n",
      "23689  22-24 ajustable bebe con cinturón botas poloni...  \n",
      "23690  12-14 turquía hombre con protección uv complet...  \n",
      "\n",
      "[23691 rows x 3 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alcoholyvendas\\AppData\\Local\\Temp\\ipykernel_17132\\3674956024.py:84: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  result = df_complete.groupby('item_id').apply(procesar_grupo).reset_index()\n"
     ]
    }
   ],
   "source": [
    "# Fase de encodeo!\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from scipy.sparse import load_npz, csr_matrix, save_npz\n",
    "from datetime import datetime\n",
    "\n",
    "def cargar_pkls():\n",
    "    try:\n",
    "        with open('./dataEncoderDecoder_item_id.pkl', 'rb') as file:\n",
    "            data = pickle.load(file)            \n",
    "        \n",
    "        digested_items = data['result']\n",
    "        X_sparse = data['X_sparse']\n",
    "        cv = data['cv']            \n",
    "        \n",
    "        # Convertir la matriz dispersa a una matriz densa si es necesario\n",
    "        X = X_sparse.toarray()\n",
    "        \n",
    "        #with open('similarity_matrix.pkl', 'rb') as f:\n",
    "        #    similarity = pickle.load(f)\n",
    "\n",
    "        # Cargar la matriz dispersa desde el archivo, más eficiente para alamecenamiento ...\n",
    "        #similarity_sparse_matrix = load_npz('./similarity_sparse.npz')\n",
    "        \n",
    "        # Convertir la matriz dispersa a densa si es necesario para el cálculo\n",
    "        #similarity = similarity_sparse_matrix.toarray()            \n",
    "        \n",
    "        return digested_items, cv, X#, similarity_sparse_matrix \n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None, None, None#, None\n",
    "    \n",
    "# Cargar los objetos desde los archivos pickle\n",
    "digested_items, cv, X = cargar_pkls()\n",
    "#similarity = cosine_similarity(X)\n",
    "\n",
    "# Ahora puedes usar `result`, `cv` y `X` como lo hacías antes\n",
    "\n",
    "if digested_items is None:\n",
    "    # Cargar el fichero CSV\n",
    "    df = pd.read_csv('./item_features_semirelleno_con_literales_actualizado.csv')\n",
    "    # df = pd.read_csv('./item_features_encoderDecoder_v1.csv')  # Usar este si se prefiere\n",
    "    # df = pd.read_csv('./item_features_encoderDecoder_v2.csv')  # Usar este si se prefiere\n",
    "\n",
    "    # Eliminar filas con NaN en 'feature_category_literal' y 'feature_value_literal'\n",
    "    df_complete = df.dropna(subset=['feature_category_literal', 'feature_value_literal'])\n",
    "\n",
    "    # Definir los valores especiales para 'descrip'\n",
    "    categorias_descriptivas = ['productos', 'materiales', 'caracteristicas', 'color', 'ocasion' , 'estaciones']\n",
    "\n",
    "    # Crear una función para procesar cada grupo de item_id\n",
    "    def procesar_grupo(grupo):\n",
    "        grupo_copia = grupo.copy()\n",
    "        # Filtrar los feature_value_literal que son considerados especiales y existen en el grupo\n",
    "        valores_especiales_en_grupo = grupo_copia[grupo_copia['feature_category_literal'].isin(categorias_descriptivas)]\n",
    "\n",
    "        # Ordenar 'valores_especiales_en_grupo' por el orden de 'categorias_descriptivas'\n",
    "        valores_especiales_en_grupo['feature_category_literal'] = pd.Categorical(\n",
    "            valores_especiales_en_grupo['feature_category_literal'], \n",
    "            categories=categorias_descriptivas,\n",
    "            ordered=True\n",
    "        )\n",
    "        valores_especiales_en_grupo = valores_especiales_en_grupo.sort_values(by='feature_category_literal')\n",
    "\n",
    "        # Concatenar los valores especiales para 'descrip' solo si existen en el item_id\n",
    "        descrip = ' '.join(valores_especiales_en_grupo['feature_value_literal'])\n",
    "        # Concatenar item_id al campo descrip con el formato '-(item_id)-'\n",
    "        descrip += ' ({})'.format(grupo_copia['item_id'].iloc[0])\n",
    "\n",
    "\n",
    "        # Crear la columna 'keywords' eliminando 'feature_category_literal' si está en 'feature_value_literal'\n",
    "        keywords = ' '.join(\n",
    "            grupo_copia['feature_value_literal'].apply(\n",
    "                lambda x: x.replace(grupo_copia['feature_category_literal'].iloc[0], '') if grupo_copia['feature_category_literal'].iloc[0] in x else x\n",
    "            ).unique()\n",
    "        ).lower()\n",
    "\n",
    "        return pd.Series({'item_descrip': descrip, 'item_keywords': keywords})\n",
    "\n",
    "    # Aplicar la función al DataFrame agrupado por 'item_id'\n",
    "    result = df_complete.groupby('item_id').apply(procesar_grupo).reset_index()\n",
    "\n",
    "    # Mostrar el resultado\n",
    "    print(result)\n",
    "\n",
    "    def collapseDouble(L):\n",
    "        if isinstance(L, list):\n",
    "            return [i.replace(\"  \", \" \") for i in L]\n",
    "        return [] \n",
    "\n",
    "    #result.loc[:, 'item_keywords'] = result['item_keywords'].apply(collapseDouble)\n",
    "    #result['item_keywords'] = result['item_keywords'].apply(lambda x:x.lower())\n",
    "    # Convertir todos los valores de 'item_keywords' a string y luego a minúsculas\n",
    "    #result['item_keywords'] = result['item_keywords'].astype(str).apply(lambda x: x.lower())\n",
    "\n",
    "\n",
    "    print(result)\n",
    "else:\n",
    "    result = digested_items\n",
    "    print(\"De datos datos cargados, X, cv, digested_items!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4b2ee85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\alcoholyvendas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos datos generados, count_vectorizer/features_matrix!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix, save_npz\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def compute_and_save_sparse_similarity(X, threshold=0.1, filename='./similarity_sparse.npz', block_size=1000):\n",
    "    \"\"\"\n",
    "    Calcula la matriz de similitud en bloques y guarda solo las similitudes que superan el umbral en formato disperso.\n",
    "    \n",
    "    Args:\n",
    "    - X: La matriz de características (muestras x características).\n",
    "    - threshold: El umbral de similitud para guardar valores.\n",
    "    - filename: El nombre del archivo para guardar la matriz dispersa.\n",
    "    - block_size: Tamaño del bloque para procesar las similitudes en partes.\n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    data = []\n",
    "    rows = []\n",
    "    cols = []\n",
    "    \n",
    "    # Procesar la matriz en bloques para evitar problemas de memoria\n",
    "    for start_row in range(0, n_samples, block_size):\n",
    "        end_row = min(start_row + block_size, n_samples)\n",
    "        # Extraer el bloque actual\n",
    "        block = X[start_row:end_row]\n",
    "        \n",
    "        # Calcular similitudes para el bloque actual\n",
    "        similarities = cosine_similarity(block, X)\n",
    "        \n",
    "        # Filtrar por el umbral y almacenar en formato disperso\n",
    "        for i in range(end_row - start_row):\n",
    "            for j in range(n_samples):\n",
    "                sim = similarities[i, j]\n",
    "                if sim > threshold:\n",
    "                    data.append(sim)\n",
    "                    rows.append(start_row + i)\n",
    "                    cols.append(j)\n",
    "    \n",
    "    # Crear la matriz dispersa\n",
    "    sparse_matrix = coo_matrix((data, (rows, cols)), shape=(n_samples, n_samples))\n",
    "    \n",
    "    # Guardar la matriz dispersa en un archivo\n",
    "    save_npz(filename, sparse_matrix)\n",
    "    print(\"Matriz dispersa de similaridad guardada!\")\n",
    "\n",
    "# Ejemplo de uso\n",
    "# X = ... (tu matriz de características)\n",
    "# compute_and_save_sparse_similarity(X)\n",
    "\n",
    "    \n",
    "if (cv is None or X is None):\n",
    "\n",
    "    ps = PorterStemmer()\n",
    "    \n",
    "    def stem(text):\n",
    "        y = []\n",
    "\n",
    "        for i in text.split():\n",
    "            y.append(ps.stem(i))\n",
    "\n",
    "        return \" \".join(y)\n",
    "\n",
    "    result['item_keywords'] = result['item_keywords'].apply(stem)\n",
    "\n",
    "    \n",
    "    from nltk.corpus import stopwords\n",
    "    # Descargar stopwords en español si no están ya disponibles\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "    # Obtener la lista de stop words en español\n",
    "    stop_words_spanish = list(stopwords.words('spanish'))\n",
    "\n",
    "    # Crear CountVectorizer con la lista de stop words en español\n",
    "    cv = CountVectorizer(max_features=5000, stop_words=stop_words_spanish)\n",
    "    #cv = CountVectorizer(max_features=5000,stop_words='english')\n",
    "\n",
    "    X = cv.fit_transform(result['item_keywords']).toarray()\n",
    "\n",
    "    #print(\"get_feature_names_out\",cv.get_feature_names_out())\n",
    "    #print(\"get_stop_words\",cv.get_stop_words())\n",
    "\n",
    "\n",
    "    #calculate the cosine distance between each vector(movie)\n",
    "    #cosine similarity is used because it expresses the similarity between the two objects\n",
    "    \n",
    "    #similarity = cosine_similarity(X)\n",
    "    # Supongamos que `similarity` es una matriz densa\n",
    "    #similarity_sparse = csr_matrix(similarity)\n",
    "\n",
    "    # Guardar la matriz dispersa en un archivo\n",
    "    #save_npz('./similarity_sparse.npz', csr_matrix(similarity))\n",
    "    #compute_and_save_sparse_similarity(X)\n",
    "    \n",
    "    \n",
    "    #print(\"similarity\",similarity)\n",
    "    #print(\"similarity[0]\",similarity[0])\n",
    "    print(\"Datos datos generados, count_vectorizer/features_matrix!\")\n",
    "else:\n",
    "    print(\"De datos datos cargados, count_vectorizer/features_matrix!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef85b5f3-fec6-4b77-b823-e73088397032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_stemmed:ahora que llega el verano, me gustaría una camisa blanca para má veranos, quizá de algodón o que me recomienda\n",
      "query_vector:[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0]]\n",
      "Palabra: algodón, Frecuencia: 1\n",
      "Palabra: blanca, Frecuencia: 1\n",
      "Palabra: verano, Frecuencia: 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>189</td>\n",
       "      <td>2024-08-31 18:10:34.602896</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   session_id  item_id                        date\n",
       "0           1      189  2024-08-31 18:10:34.602896"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def compute_query_similarity(query, X, cv, threshold=0.1):\n",
    "    \"\"\"\n",
    "    Calcula la similitud entre la consulta y todos los ítems existentes usando una matriz dispersa guardada.\n",
    "    \n",
    "    Args:\n",
    "    - query: La consulta para calcular similitudes.\n",
    "    - X: La matriz de características (muestras x características).\n",
    "    - cv: El CountVectorizer o TF-IDF Vectorizer utilizado para transformar la consulta.\n",
    "    - sparse_matrix_filename: El nombre del archivo que contiene la matriz dispersa de similitud.\n",
    "    - threshold: El umbral de similitud para considerar valores relevantes.\n",
    "    \n",
    "    Returns:\n",
    "    - sparse_matrix: La matriz dispersa de similitudes.\n",
    "    - query_similarity: La similitud de la consulta con todos los ítems.\n",
    "    \"\"\"\n",
    "    ps = PorterStemmer()\n",
    "    \n",
    "    def stem(text):\n",
    "        y = []\n",
    "\n",
    "        for i in text.split():\n",
    "            y.append(ps.stem(i))\n",
    "\n",
    "        return \" \".join(y)\n",
    "    \n",
    "    query_stemmed = stem(query.lower())\n",
    "    print(f\"query_stemmed:{query_stemmed}\")\n",
    "    \n",
    "    # Transformar la consulta en un vector\n",
    "    query_vector = cv.transform([query_stemmed]).toarray()\n",
    "    print(f\"query_vector:{query_vector}\")\n",
    "    \n",
    "    # Obtener el vocabulario del CountVectorizer\n",
    "    vocab = cv.vocabulary_\n",
    "\n",
    "    # Invertir el diccionario de vocabulario para obtener un mapeo de índices a palabras\n",
    "    index_to_word = {v: k for k, v in vocab.items()}\n",
    "\n",
    "    # Mostrar las palabras correspondientes a los índices con valores distintos de cero\n",
    "    for index, value in enumerate(query_vector[0]):\n",
    "        if value > 0:\n",
    "            print(f\"Palabra: {index_to_word[index]}, Frecuencia: {value}\")    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Cargar la matriz dispersa desde el archivo\n",
    "    #sparse_matrix = load_npz(sparse_matrix_filename)\n",
    "    \n",
    "    \n",
    "    # Calcular la similitud entre la consulta y todos los ítems\n",
    "    query_similarity = cosine_similarity(query_vector, X).flatten()\n",
    "    \n",
    "    # Filtrar similitudes que superan el umbral\n",
    "    data = []\n",
    "    rows = []\n",
    "    cols = []\n",
    "    for i, sim in enumerate(query_similarity):\n",
    "        if sim > threshold:\n",
    "            data.append(sim)\n",
    "            rows.append(0)  # Fila 0 para la consulta\n",
    "            cols.append(i)\n",
    "    \n",
    "    # Crear la matriz dispersa de similitudes para la consulta\n",
    "    query_sparse_matrix = coo_matrix((data, (rows, cols)), shape=(1, X.shape[0]))\n",
    "    \n",
    "    return query_sparse_matrix, query_similarity\n",
    "\n",
    "def get_top_items(query_sparse_matrix, query_similarity, top_n=5):\n",
    "    \"\"\"\n",
    "    Obtiene los índices de los ítems más similares desde la matriz dispersa.\n",
    "    \n",
    "    Args:\n",
    "    - query_sparse_matrix: La matriz dispersa de similitudes para la consulta.\n",
    "    - query_similarity: La similitud de la consulta con todos los ítems.\n",
    "    - top_n: Número de ítems más similares a recuperar.\n",
    "    \n",
    "    Returns:\n",
    "    - items_list: Lista de los ítems más similares.\n",
    "    \"\"\"\n",
    "    # Convertir la matriz dispersa a densa si es necesario\n",
    "    dense_similarity = query_sparse_matrix.toarray().flatten()\n",
    "    \n",
    "    # Obtener los índices de los ítems más similares\n",
    "    item_indices = np.argsort(-dense_similarity)[:top_n]\n",
    "    items_list = [(i, dense_similarity[i]) for i in item_indices]\n",
    "    \n",
    "    return items_list\n",
    "\n",
    "# Esto genera un candidato a dar al recomendador, todas las preguntas podrían irse acumulando en la misma sesión\n",
    "# para ir acumulando las queries e ir pasandolas todas cada vez que hay una nueva!\n",
    "def encoding2recomender(yourQuery, session_id=1):\n",
    "\n",
    "    query_sparse_matrix, query_similarity = compute_query_similarity(yourQuery, X, cv)\n",
    "    items_list = get_top_items(query_sparse_matrix, query_similarity)    \n",
    "\n",
    "    candidate_df = None\n",
    "    # Crear la salida en formato CSV con solo el primer ítem\n",
    "    if items_list:  # Verificar que hay elementos en items_list\n",
    "        first_item_index = items_list[0][0]  # Obtener el índice del primer ítem\n",
    "        item_id = result.iloc[first_item_index].item_id\n",
    "        candidates = [{\n",
    "            'session_id': session_id,\n",
    "            'item_id': item_id,\n",
    "            'date': datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')  # Formatear la fecha\n",
    "        }]\n",
    "        \n",
    "        # Convertir el resultado a un DataFrame\n",
    "        candidate_df = pd.DataFrame(candidates)\n",
    "    else:\n",
    "        # Devolver un DataFrame vacío si items_list está vacío\n",
    "        candidate_df = pd.DataFrame(columns=['session_id', 'item_id', 'date'])        \n",
    "    \n",
    "    return candidate_df        \n",
    "        \n",
    "        \n",
    "# Ejemplo de uso\n",
    "encoding2recomender(\"Ahora que llega el verano, me gustaría una camisa blanca para más veranos, quizá de algodón o que me recomiendas\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d37fb32-7d5a-4853-a1db-48c9c601b45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from scipy.sparse import csr_matrix, save_npz\n",
    "\n",
    "X_sparse = csr_matrix(X)\n",
    "data = {\n",
    "    'result': result,\n",
    "    'X_sparse': X_sparse,\n",
    "    'cv': cv\n",
    "}\n",
    "# Guardar el diccionario en un archivo pickle\n",
    "with open('./dataEncoderDecoder_item_id.pkl', 'wb') as file:\n",
    "    pickle.dump(data, file)\n",
    "\n",
    "# Esto es lo que hay que cargar en la api function, además del modelo ...\n",
    "# el enconding2recomender\n",
    "#pickle.dump(result, open('./digested_items.pkl', 'wb'))\n",
    "#pickle.dump(cv, open('./count_vectorizer.pkl', 'wb'))\n",
    "#pickle.dump(X, open('./features_matrix.pkl', 'wb'))\n",
    "\n",
    "# Apartir de aquí, creo que no haría falta cargarlo! (la similaridad se hace por el par X -matriz dispersa- y la query)\n",
    "#pickle.dump(result.to_dict(), open('./digested_items_dict.pkl', 'wb'))\n",
    "#pickle.dump(similarity, open('./similarity_matrix.pkl', 'wb'))\n",
    "#np.save('similarity_matrix.npy', similarity)  # Guarda en disco, si sólo hay que guardar eso!\n",
    "#Pero si hay problemas de tamaño, lo siguiente es lo mejor!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b7c5b7f2-4470-447f-b2d0-70065703d06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Cazadora Seda Completo Casual Primavera (3260)\", \"Americana Seda Completo Casual Verano (5383)\", \"Camiseta Seda Amortiguada Casual Primavera (26538)\", \"Leggings Seda Completo Casual Verano (20541)\", \"Cazadora Seda Completo Casual Verano (2213)\", \"Camiseta Seda Corto Casual Verano (6603)\", \"Cazadora Seda Corto Casual Verano (27921)\", \"Camiseta Nylon Elástico Casual Verano (3425)\", \"Cazadora Seda Corto Casual Verano (7368)\", \"Camiseta Cuero Completo Casual Primavera (7719)\"]\n",
      "26 [\"Cazadora Seda Completo Casual Primavera (3260)\", \"Americana Seda Completo Casual Verano (5383)\", \"Camiseta Seda Amortiguada Casual Primavera (26538)\", \"Leggings Seda Completo Casual Verano (20541)\", \"Cazadora Seda Completo Casual Verano (2213)\", \"Camiseta Seda Corto Casual Verano (6603)\", \"Cazadora Seda Corto Casual Verano (27921)\", \"Camiseta Nylon Elástico Casual Verano (3425)\", \"Cazadora Seda Corto Casual Verano (7368)\", \"Camiseta Cuero Completo Casual Primavera (7719)\"]\n",
      "61 [\"Zapatillas de casa Cuero Completo Casual Verano (25956)\", \"Bailarinas Cuero Amortiguada Casual Verano (13936)\", \"Americana Cuero Manga corta Viaje Verano (16877)\", \"Mono Cuero Completo Eventos Deportivos Verano (7225)\", \"Zuecos Seda Largo Casual Verano (11641)\", \"Gafas de sol Seda Manga corta Casual Verano (14075)\", \"Sudadera Cuero Manga corta Casual Otoño (20236)\", \"Bikini Cuero Manga corta Casual Verano (18298)\", \"Sudadera Cuero Completo Casual Primavera (10207)\", \"Camiseta Plástico Corto Casual Verano (14455)\"]\n"
     ]
    }
   ],
   "source": [
    "# Fase de decodificación!\n",
    "# Si esto es la salida del recomendador, hay de decodearlo para devolverlo al usuario (respuesta del chat)\n",
    "# Siempre será una sesión nada más, pero para ver variedad!\n",
    "prediccion_df = pd.DataFrame({\n",
    "    'session_id': [26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61],\n",
    "    'item_id': [3260, 5383, 26538, 20541, 2213, 6603, 27921, 3425, 7368, 7719, 25956, 13936, 16877, 7225, 11641, 14075, 20236, 18298, 10207, 14455],\n",
    "    'rank': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "})\n",
    "\n",
    "def decoding2chat(prediccion_df):\n",
    "    \"\"\"\n",
    "    Añade la columna 'item_descrip' a prediccion_df basándose en el DataFrame result.\n",
    "    \n",
    "    Parameters:\n",
    "    prediccion_df (pd.DataFrame): DataFrame con columnas 'session_id', 'item_id', 'rank'.\n",
    "    result (pd.DataFrame): DataFrame con las columnas 'item_id', 'item_descrip', 'item_keywords'.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: prediccion_df con la nueva columna 'item_descrip'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Crear un diccionario para la búsqueda rápida\n",
    "    item_descrip_dict = result.set_index('item_id')['item_descrip'].to_dict()\n",
    "\n",
    "    # Función para obtener la descripción del item\n",
    "    def obtener_descripcion(item_id):\n",
    "        return item_descrip_dict.get(item_id, '')\n",
    "\n",
    "    # Aplicar la función a cada item_id en prediccion_df\n",
    "    prediccion_df['item_descrip'] = prediccion_df['item_id'].apply(obtener_descripcion)\n",
    "\n",
    "    #return prediccion_df\n",
    "    #joined_df = decoded_prediccion_df.groupby('session_id')['item_descrip'].apply(lambda x: ', '.join(x)).reset_index()\n",
    "    listed_df = prediccion_df.groupby('session_id')['item_descrip'].apply(list).reset_index()\n",
    "    \n",
    "    return listed_df\n",
    "\n",
    "# Mostrar el DataFrame final con la columna añadida\n",
    "decoded_prediccion_df = decoding2chat(prediccion_df)\n",
    "#print(decoded_prediccion_df)\n",
    "\n",
    "import json\n",
    "\n",
    "# Mostrar el DataFrame de 'decoded_prediccion_df'\n",
    "# Acceder a la primera fila de listed_df al valor 'item_descrip'\n",
    "#lista = decoded_prediccion_df.iloc[0]\n",
    "#print(lista)\n",
    "print(json.dumps(decoded_prediccion_df.iloc[0]['item_descrip'], ensure_ascii=False))\n",
    "\n",
    "# Impresión si quieres, iterar por cada fila en 'listed_df' y convertirla a JSON\n",
    "for index, row in decoded_prediccion_df.iterrows():\n",
    "    session_id = row['session_id']\n",
    "    item_descriptions = row['item_descrip']\n",
    "    \n",
    "    # Crear un diccionario para representar la fila\n",
    "    data = {\n",
    "        'session_id': session_id,\n",
    "        'item_descrip': item_descriptions\n",
    "    }\n",
    "    \n",
    "    # Convertir el diccionario a JSON y imprimirlo\n",
    "    json_data = json.dumps(item_descriptions, ensure_ascii=False)\n",
    "    print(session_id, json_data)\n",
    "\n",
    "\n",
    "#listed_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c73848",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e0c2bd-bb3b-4472-9ab3-7f7ddb95dae8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
